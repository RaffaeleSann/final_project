{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run PSO.ipynb #da qui possiamousare la funzione PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D,Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
      "2110848/2110848 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=10000, test_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982 train sequences\n",
      "2246 test sequences\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 classes\n"
     ]
    }
   ],
   "source": [
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing sequence data...\n",
      "x_train shape: (8982, 10000)\n",
      "x_test shape: (2246, 10000)\n"
     ]
    }
   ],
   "source": [
    "print('Vectorizing sequence data...')\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert class vector to binary class matrix for use with categorical_crossentropy\n",
      "y_train shape: (8982, 46)\n",
      "y_test shape: (2246, 46)\n"
     ]
    }
   ],
   "source": [
    "print('Convert class vector to binary class matrix for use with categorical_crossentropy')\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "def MLP(particle_position):\n",
    "    '''This function returns the accuracy of a neural network model with the parameter set as the component of the particle position.\n",
    "     The first component of the position will be the batch size, the second willl be the optimizer and the third will be the learning rate. \n",
    "     particle_position[0]: batch_size\n",
    "     particle_position[1]: optimizer (<1: SGD; within [1,2): SGD with Momentum; within [2,3): Adam; within [3,inf]: RMSPROP )\n",
    "     particle_position[2]: learning_rate'''\n",
    "    \n",
    "    batch_s = particle_position[0]\n",
    "    batch_s = int(batch_s)\n",
    "    optimiz = particle_position[1]\n",
    "    lr = particle_position[2]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(10000,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss',patience=3, verbose=1, mode='min', baseline=None)\n",
    "    callbacks = [early_stopping]\n",
    "\n",
    "    if optimiz < 1:\n",
    "        #use SGD\n",
    "        opt = SGD(learning_rate = lr)\n",
    "        \n",
    "    elif optimiz >=1 and optimiz < 2:\n",
    "        #use SGD with Momentum\n",
    "        opt = SGD(momentum = 0.9, learning_rate = lr)\n",
    "\n",
    "    elif optimiz >= 2 and optimiz < 3:\n",
    "        #use ADAM\n",
    "        opt = Adam(learning_rate = lr)\n",
    "\n",
    "    else:\n",
    "        #use RMSPROP\n",
    "        opt = RMSprop(learning_rate = lr)\n",
    "\n",
    "    model.compile(optimizer= opt,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(x_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=batch_s,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=callbacks)\n",
    "    \n",
    "    last_accuracy = history.history['val_accuracy'][-1]\n",
    "\n",
    "    return last_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"plt.plot(history.history['loss'], label='Training loss')\\nplt.plot(history.history['val_loss'], label='Validation loss')\\nplt.title('Training and validation loss')\\nplt.xlabel('Epochs')\\nplt.ylabel('Loss')\\nplt.legend()\\nplt.show()\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"plt.plot(history.history['accuracy'], label='Training accuracy')\\nplt.plot(history.history['val_accuracy'], label='Validation accuracy')\\nplt.title('Training and validation accuracy')\\nplt.xlabel('Epochs')\\nplt.ylabel('Accuracy')\\nplt.legend()\\nplt.show()\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''plt.plot(history.history['accuracy'], label='Training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 3.6954 - accuracy: 0.2046 - val_loss: 3.5266 - val_accuracy: 0.4686\n",
      "Epoch 2/20\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 3.3671 - accuracy: 0.4501 - val_loss: 3.1629 - val_accuracy: 0.4880\n",
      "Epoch 3/20\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 2.9931 - accuracy: 0.4597 - val_loss: 2.7968 - val_accuracy: 0.4814\n",
      "Epoch 4/20\n",
      "79/79 [==============================] - 2s 26ms/step - loss: 2.6796 - accuracy: 0.4637 - val_loss: 2.5332 - val_accuracy: 0.4914\n",
      "Epoch 5/20\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 2.4688 - accuracy: 0.4784 - val_loss: 2.3631 - val_accuracy: 0.5120\n",
      "Epoch 6/20\n",
      "79/79 [==============================] - 2s 26ms/step - loss: 2.3277 - accuracy: 0.4945 - val_loss: 2.2396 - val_accuracy: 0.5225\n",
      "Epoch 7/20\n",
      "79/79 [==============================] - 2s 26ms/step - loss: 2.2194 - accuracy: 0.5044 - val_loss: 2.1437 - val_accuracy: 0.5231\n",
      "Epoch 8/20\n",
      "79/79 [==============================] - 2s 26ms/step - loss: 2.1358 - accuracy: 0.5097 - val_loss: 2.0668 - val_accuracy: 0.5237\n",
      "Epoch 9/20\n",
      "79/79 [==============================] - 2s 26ms/step - loss: 2.0663 - accuracy: 0.5143 - val_loss: 2.0036 - val_accuracy: 0.5253\n",
      "Epoch 10/20\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 2.0097 - accuracy: 0.5194 - val_loss: 1.9523 - val_accuracy: 0.5237\n",
      "Epoch 11/20\n",
      "79/79 [==============================] - 2s 26ms/step - loss: 1.9648 - accuracy: 0.5237 - val_loss: 1.9089 - val_accuracy: 0.5287\n",
      "Epoch 12/20\n",
      "79/79 [==============================] - 2s 26ms/step - loss: 1.9163 - accuracy: 0.5360 - val_loss: 1.8713 - val_accuracy: 0.5348\n",
      "Epoch 13/20\n",
      "79/79 [==============================] - 2s 26ms/step - loss: 1.8804 - accuracy: 0.5403 - val_loss: 1.8391 - val_accuracy: 0.5403\n",
      "Epoch 14/20\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 1.8550 - accuracy: 0.5485 - val_loss: 1.8105 - val_accuracy: 0.5431\n",
      "Epoch 15/20\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 1.8263 - accuracy: 0.5541 - val_loss: 1.7848 - val_accuracy: 0.5481\n",
      "Epoch 16/20\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 1.7993 - accuracy: 0.5571 - val_loss: 1.7619 - val_accuracy: 0.5682\n",
      "Epoch 17/20\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 1.7778 - accuracy: 0.5670 - val_loss: 1.7409 - val_accuracy: 0.5760\n",
      "Epoch 18/20\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 1.7616 - accuracy: 0.5731 - val_loss: 1.7208 - val_accuracy: 0.5810\n",
      "Epoch 19/20\n",
      "79/79 [==============================] - 2s 26ms/step - loss: 1.7408 - accuracy: 0.5809 - val_loss: 1.7020 - val_accuracy: 0.5865\n",
      "Epoch 20/20\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 1.7149 - accuracy: 0.5868 - val_loss: 1.6846 - val_accuracy: 0.5954\n",
      "Epoch 1/20\n",
      "225/225 [==============================] - 9s 35ms/step - loss: 1.2443 - accuracy: 0.7304 - val_loss: 0.8961 - val_accuracy: 0.8013\n",
      "Epoch 2/20\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.5860 - accuracy: 0.8736 - val_loss: 0.9103 - val_accuracy: 0.8114\n",
      "Epoch 3/20\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.4054 - accuracy: 0.9144 - val_loss: 1.0051 - val_accuracy: 0.8097\n",
      "Epoch 4/20\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.3900 - accuracy: 0.9346 - val_loss: 1.1069 - val_accuracy: 0.8024\n",
      "Epoch 4: early stopping\n",
      "Epoch 1/20\n",
      "80/80 [==============================] - 7s 79ms/step - loss: 1.2833 - accuracy: 0.7215 - val_loss: 0.9181 - val_accuracy: 0.7924\n",
      "Epoch 2/20\n",
      "80/80 [==============================] - 6s 77ms/step - loss: 0.5570 - accuracy: 0.8782 - val_loss: 0.8095 - val_accuracy: 0.8314\n",
      "Epoch 3/20\n",
      "80/80 [==============================] - 6s 75ms/step - loss: 0.3379 - accuracy: 0.9241 - val_loss: 0.9468 - val_accuracy: 0.8114\n",
      "Epoch 4/20\n",
      "80/80 [==============================] - 6s 74ms/step - loss: 0.2376 - accuracy: 0.9445 - val_loss: 1.0376 - val_accuracy: 0.8125\n",
      "Epoch 5/20\n",
      "80/80 [==============================] - 6s 74ms/step - loss: 0.2140 - accuracy: 0.9523 - val_loss: 1.1642 - val_accuracy: 0.8086\n",
      "Epoch 5: early stopping\n",
      "Epoch 1/20\n",
      "105/105 [==============================] - 9s 55ms/step - loss: 2.5177 - accuracy: 0.4554 - val_loss: 1.8095 - val_accuracy: 0.5531\n",
      "Epoch 2/20\n",
      "105/105 [==============================] - 5s 47ms/step - loss: 1.7041 - accuracy: 0.5921 - val_loss: 1.5668 - val_accuracy: 0.6377\n",
      "Epoch 3/20\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 1.5024 - accuracy: 0.6533 - val_loss: 1.4276 - val_accuracy: 0.6795\n",
      "Epoch 4/20\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 1.3637 - accuracy: 0.6896 - val_loss: 1.3292 - val_accuracy: 0.7040\n",
      "Epoch 5/20\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 1.2588 - accuracy: 0.7151 - val_loss: 1.2613 - val_accuracy: 0.7195\n",
      "Epoch 6/20\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 1.1720 - accuracy: 0.7340 - val_loss: 1.2058 - val_accuracy: 0.7351\n",
      "Epoch 7/20\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 1.1033 - accuracy: 0.7500 - val_loss: 1.1638 - val_accuracy: 0.7457\n",
      "Epoch 8/20\n",
      "105/105 [==============================] - 4s 41ms/step - loss: 1.0402 - accuracy: 0.7660 - val_loss: 1.1280 - val_accuracy: 0.7529\n",
      "Epoch 9/20\n",
      "105/105 [==============================] - 4s 42ms/step - loss: 0.9889 - accuracy: 0.7811 - val_loss: 1.0944 - val_accuracy: 0.7568\n",
      "Epoch 10/20\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.9338 - accuracy: 0.7878 - val_loss: 1.0639 - val_accuracy: 0.7629\n",
      "Epoch 11/20\n",
      "105/105 [==============================] - 4s 35ms/step - loss: 0.8897 - accuracy: 0.8018 - val_loss: 1.0420 - val_accuracy: 0.7691\n",
      "Epoch 12/20\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 0.8477 - accuracy: 0.8113 - val_loss: 1.0203 - val_accuracy: 0.7752\n",
      "Epoch 13/20\n",
      "105/105 [==============================] - 4s 39ms/step - loss: 0.8193 - accuracy: 0.8143 - val_loss: 1.0047 - val_accuracy: 0.7774\n",
      "Epoch 14/20\n",
      "105/105 [==============================] - 4s 39ms/step - loss: 0.7745 - accuracy: 0.8267 - val_loss: 0.9837 - val_accuracy: 0.7813\n",
      "Epoch 15/20\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.7423 - accuracy: 0.8335 - val_loss: 0.9701 - val_accuracy: 0.7830\n",
      "Epoch 16/20\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 0.7132 - accuracy: 0.8398 - val_loss: 0.9589 - val_accuracy: 0.7835\n",
      "Epoch 17/20\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 0.6846 - accuracy: 0.8424 - val_loss: 0.9464 - val_accuracy: 0.7869\n",
      "Epoch 18/20\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.6623 - accuracy: 0.8541 - val_loss: 0.9330 - val_accuracy: 0.7913\n",
      "Epoch 19/20\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 0.6352 - accuracy: 0.8585 - val_loss: 0.9242 - val_accuracy: 0.7930\n",
      "Epoch 20/20\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 0.6080 - accuracy: 0.8640 - val_loss: 0.9128 - val_accuracy: 0.7952\n",
      "Epoch 1/20\n",
      "225/225 [==============================] - 12s 42ms/step - loss: 1.4563 - accuracy: 0.6931 - val_loss: 1.1108 - val_accuracy: 0.7774\n",
      "Epoch 2/20\n",
      "225/225 [==============================] - 9s 40ms/step - loss: 1.0390 - accuracy: 0.7894 - val_loss: 1.2459 - val_accuracy: 0.7546\n",
      "Epoch 3/20\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.9738 - accuracy: 0.8245 - val_loss: 1.2586 - val_accuracy: 0.7896\n",
      "Epoch 4/20\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.8928 - accuracy: 0.8395 - val_loss: 1.4223 - val_accuracy: 0.7774\n",
      "Epoch 4: early stopping\n",
      "Epoch 1/20\n",
      "423/423 [==============================] - 13s 24ms/step - loss: 2.4361 - accuracy: 0.4565 - val_loss: 1.8510 - val_accuracy: 0.5342\n",
      "Epoch 2/20\n",
      "423/423 [==============================] - 10s 24ms/step - loss: 1.7482 - accuracy: 0.5702 - val_loss: 1.6181 - val_accuracy: 0.6311\n",
      "Epoch 3/20\n",
      "423/423 [==============================] - 9s 22ms/step - loss: 1.5574 - accuracy: 0.6345 - val_loss: 1.4812 - val_accuracy: 0.6700\n",
      "Epoch 4/20\n",
      "423/423 [==============================] - 9s 22ms/step - loss: 1.4263 - accuracy: 0.6724 - val_loss: 1.3870 - val_accuracy: 0.6878\n",
      "Epoch 5/20\n",
      "423/423 [==============================] - 9s 22ms/step - loss: 1.3211 - accuracy: 0.7035 - val_loss: 1.3164 - val_accuracy: 0.7073\n",
      "Epoch 6/20\n",
      "423/423 [==============================] - 9s 22ms/step - loss: 1.2462 - accuracy: 0.7225 - val_loss: 1.2584 - val_accuracy: 0.7229\n",
      "Epoch 7/20\n",
      "423/423 [==============================] - 10s 23ms/step - loss: 1.1752 - accuracy: 0.7385 - val_loss: 1.2110 - val_accuracy: 0.7351\n",
      "Epoch 8/20\n",
      "423/423 [==============================] - 9s 22ms/step - loss: 1.1083 - accuracy: 0.7525 - val_loss: 1.1728 - val_accuracy: 0.7490\n",
      "Epoch 9/20\n",
      "423/423 [==============================] - 10s 23ms/step - loss: 1.0626 - accuracy: 0.7628 - val_loss: 1.1420 - val_accuracy: 0.7518\n",
      "Epoch 10/20\n",
      "423/423 [==============================] - 9s 21ms/step - loss: 1.0122 - accuracy: 0.7770 - val_loss: 1.1090 - val_accuracy: 0.7624\n",
      "Epoch 11/20\n",
      "423/423 [==============================] - 8s 20ms/step - loss: 0.9721 - accuracy: 0.7836 - val_loss: 1.0874 - val_accuracy: 0.7613\n",
      "Epoch 12/20\n",
      "423/423 [==============================] - 10s 24ms/step - loss: 0.9290 - accuracy: 0.7935 - val_loss: 1.0645 - val_accuracy: 0.7652\n",
      "Epoch 13/20\n",
      "423/423 [==============================] - 13s 31ms/step - loss: 0.8909 - accuracy: 0.8026 - val_loss: 1.0447 - val_accuracy: 0.7707\n",
      "Epoch 14/20\n",
      "423/423 [==============================] - 12s 29ms/step - loss: 0.8607 - accuracy: 0.8051 - val_loss: 1.0260 - val_accuracy: 0.7718\n",
      "Epoch 15/20\n",
      "423/423 [==============================] - 13s 30ms/step - loss: 0.8279 - accuracy: 0.8171 - val_loss: 1.0111 - val_accuracy: 0.7769\n",
      "Epoch 16/20\n",
      "423/423 [==============================] - 13s 30ms/step - loss: 0.8010 - accuracy: 0.8227 - val_loss: 0.9978 - val_accuracy: 0.7824\n",
      "Epoch 17/20\n",
      "423/423 [==============================] - 13s 30ms/step - loss: 0.7711 - accuracy: 0.8245 - val_loss: 0.9842 - val_accuracy: 0.7802\n",
      "Epoch 18/20\n",
      "423/423 [==============================] - 13s 30ms/step - loss: 0.7417 - accuracy: 0.8316 - val_loss: 0.9718 - val_accuracy: 0.7841\n",
      "Epoch 19/20\n",
      "423/423 [==============================] - 13s 30ms/step - loss: 0.7116 - accuracy: 0.8402 - val_loss: 0.9594 - val_accuracy: 0.7874\n",
      "Epoch 20/20\n",
      "423/423 [==============================] - 13s 30ms/step - loss: 0.6946 - accuracy: 0.8459 - val_loss: 0.9493 - val_accuracy: 0.7913\n",
      "Epoch 1/20\n",
      "77/77 [==============================] - 18s 138ms/step - loss: 3.2372 - accuracy: 0.3353 - val_loss: 2.6162 - val_accuracy: 0.3790\n",
      "Epoch 2/20\n",
      "77/77 [==============================] - 9s 116ms/step - loss: 2.3532 - accuracy: 0.4679 - val_loss: 2.1328 - val_accuracy: 0.5181\n",
      "Epoch 3/20\n",
      "77/77 [==============================] - 5s 71ms/step - loss: 2.0481 - accuracy: 0.5125 - val_loss: 1.9255 - val_accuracy: 0.5259\n",
      "Epoch 4/20\n",
      "77/77 [==============================] - 5s 71ms/step - loss: 1.8973 - accuracy: 0.5310 - val_loss: 1.8079 - val_accuracy: 0.5454\n",
      "Epoch 5/20\n",
      "77/77 [==============================] - 5s 72ms/step - loss: 1.7949 - accuracy: 0.5532 - val_loss: 1.7264 - val_accuracy: 0.5832\n",
      "Epoch 6/20\n",
      "77/77 [==============================] - 5s 72ms/step - loss: 1.7160 - accuracy: 0.5839 - val_loss: 1.6640 - val_accuracy: 0.6027\n",
      "Epoch 7/20\n",
      "77/77 [==============================] - 5s 71ms/step - loss: 1.6619 - accuracy: 0.6045 - val_loss: 1.6136 - val_accuracy: 0.6249\n",
      "Epoch 8/20\n",
      "77/77 [==============================] - 6s 82ms/step - loss: 1.6033 - accuracy: 0.6202 - val_loss: 1.5698 - val_accuracy: 0.6433\n",
      "Epoch 9/20\n",
      "77/77 [==============================] - 6s 79ms/step - loss: 1.5589 - accuracy: 0.6347 - val_loss: 1.5319 - val_accuracy: 0.6539\n",
      "Epoch 10/20\n",
      "77/77 [==============================] - 6s 75ms/step - loss: 1.5167 - accuracy: 0.6498 - val_loss: 1.4976 - val_accuracy: 0.6583\n",
      "Epoch 11/20\n",
      "77/77 [==============================] - 4s 46ms/step - loss: 1.4784 - accuracy: 0.6559 - val_loss: 1.4661 - val_accuracy: 0.6722\n",
      "Epoch 12/20\n",
      "77/77 [==============================] - 3s 34ms/step - loss: 1.4428 - accuracy: 0.6686 - val_loss: 1.4368 - val_accuracy: 0.6789\n",
      "Epoch 13/20\n",
      "77/77 [==============================] - 3s 36ms/step - loss: 1.4139 - accuracy: 0.6793 - val_loss: 1.4118 - val_accuracy: 0.6822\n",
      "Epoch 14/20\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 1.3828 - accuracy: 0.6827 - val_loss: 1.3878 - val_accuracy: 0.6884\n",
      "Epoch 15/20\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 1.3542 - accuracy: 0.6930 - val_loss: 1.3652 - val_accuracy: 0.6923\n",
      "Epoch 16/20\n",
      "77/77 [==============================] - 5s 67ms/step - loss: 1.3312 - accuracy: 0.6960 - val_loss: 1.3462 - val_accuracy: 0.6950\n",
      "Epoch 17/20\n",
      "77/77 [==============================] - 6s 72ms/step - loss: 1.2988 - accuracy: 0.7044 - val_loss: 1.3263 - val_accuracy: 0.7040\n",
      "Epoch 18/20\n",
      "77/77 [==============================] - 5s 68ms/step - loss: 1.2786 - accuracy: 0.7129 - val_loss: 1.3081 - val_accuracy: 0.7078\n",
      "Epoch 19/20\n",
      "77/77 [==============================] - 5s 69ms/step - loss: 1.2547 - accuracy: 0.7186 - val_loss: 1.2910 - val_accuracy: 0.7101\n",
      "Epoch 20/20\n",
      "77/77 [==============================] - 5s 69ms/step - loss: 1.2349 - accuracy: 0.7222 - val_loss: 1.2763 - val_accuracy: 0.7134\n",
      "Epoch 1/20\n",
      "65/65 [==============================] - 17s 158ms/step - loss: 3.7523 - accuracy: 0.1491 - val_loss: 3.6217 - val_accuracy: 0.4613\n",
      "Epoch 2/20\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 3.5125 - accuracy: 0.4392 - val_loss: 3.3734 - val_accuracy: 0.4791\n",
      "Epoch 3/20\n",
      "65/65 [==============================] - 8s 129ms/step - loss: 3.2508 - accuracy: 0.4564 - val_loss: 3.1019 - val_accuracy: 0.4730\n",
      "Epoch 4/20\n",
      "65/65 [==============================] - 8s 131ms/step - loss: 2.9779 - accuracy: 0.4536 - val_loss: 2.8389 - val_accuracy: 0.4530\n",
      "Epoch 5/20\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 2.7405 - accuracy: 0.4429 - val_loss: 2.6260 - val_accuracy: 0.4569\n",
      "Epoch 6/20\n",
      "65/65 [==============================] - 5s 78ms/step - loss: 2.5569 - accuracy: 0.4594 - val_loss: 2.4710 - val_accuracy: 0.4836\n",
      "Epoch 7/20\n",
      "65/65 [==============================] - 5s 77ms/step - loss: 2.4310 - accuracy: 0.4759 - val_loss: 2.3557 - val_accuracy: 0.5058\n",
      "Epoch 8/20\n",
      "65/65 [==============================] - 5s 79ms/step - loss: 2.3280 - accuracy: 0.4917 - val_loss: 2.2643 - val_accuracy: 0.5109\n",
      "Epoch 9/20\n",
      "65/65 [==============================] - 5s 78ms/step - loss: 2.2500 - accuracy: 0.4987 - val_loss: 2.1896 - val_accuracy: 0.5198\n",
      "Epoch 10/20\n",
      "65/65 [==============================] - 5s 79ms/step - loss: 2.1799 - accuracy: 0.5010 - val_loss: 2.1270 - val_accuracy: 0.5248\n",
      "Epoch 11/20\n",
      "65/65 [==============================] - 5s 79ms/step - loss: 2.1233 - accuracy: 0.5100 - val_loss: 2.0734 - val_accuracy: 0.5237\n",
      "Epoch 12/20\n",
      "65/65 [==============================] - 5s 79ms/step - loss: 2.0737 - accuracy: 0.5113 - val_loss: 2.0274 - val_accuracy: 0.5231\n",
      "Epoch 13/20\n",
      "65/65 [==============================] - 5s 78ms/step - loss: 2.0348 - accuracy: 0.5145 - val_loss: 1.9871 - val_accuracy: 0.5203\n",
      "Epoch 14/20\n",
      "65/65 [==============================] - 5s 79ms/step - loss: 1.9979 - accuracy: 0.5208 - val_loss: 1.9522 - val_accuracy: 0.5220\n",
      "Epoch 15/20\n",
      "65/65 [==============================] - 5s 79ms/step - loss: 1.9679 - accuracy: 0.5209 - val_loss: 1.9219 - val_accuracy: 0.5248\n",
      "Epoch 16/20\n",
      "65/65 [==============================] - 5s 80ms/step - loss: 1.9451 - accuracy: 0.5250 - val_loss: 1.8950 - val_accuracy: 0.5248\n",
      "Epoch 17/20\n",
      "65/65 [==============================] - 5s 80ms/step - loss: 1.9098 - accuracy: 0.5331 - val_loss: 1.8705 - val_accuracy: 0.5337\n",
      "Epoch 18/20\n",
      "65/65 [==============================] - 5s 78ms/step - loss: 1.8902 - accuracy: 0.5336 - val_loss: 1.8485 - val_accuracy: 0.5381\n",
      "Epoch 19/20\n",
      "65/65 [==============================] - 5s 77ms/step - loss: 1.8684 - accuracy: 0.5404 - val_loss: 1.8284 - val_accuracy: 0.5448\n",
      "Epoch 20/20\n",
      "65/65 [==============================] - 5s 78ms/step - loss: 1.8452 - accuracy: 0.5473 - val_loss: 1.8094 - val_accuracy: 0.5487\n",
      "Epoch 1/20\n",
      "60/60 [==============================] - 17s 169ms/step - loss: 3.6983 - accuracy: 0.2051 - val_loss: 3.5450 - val_accuracy: 0.4641\n",
      "Epoch 2/20\n",
      "60/60 [==============================] - 8s 139ms/step - loss: 3.3953 - accuracy: 0.4490 - val_loss: 3.2223 - val_accuracy: 0.4758\n",
      "Epoch 3/20\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 3.0647 - accuracy: 0.4585 - val_loss: 2.8840 - val_accuracy: 0.4780\n",
      "Epoch 4/20\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 2.7554 - accuracy: 0.4612 - val_loss: 2.6133 - val_accuracy: 0.4819\n",
      "Epoch 5/20\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 2.5311 - accuracy: 0.4774 - val_loss: 2.4308 - val_accuracy: 0.4992\n",
      "Epoch 6/20\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 2.3851 - accuracy: 0.4856 - val_loss: 2.3019 - val_accuracy: 0.5170\n",
      "Epoch 7/20\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 2.2738 - accuracy: 0.4980 - val_loss: 2.2016 - val_accuracy: 0.5231\n",
      "Epoch 8/20\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 2.1818 - accuracy: 0.5062 - val_loss: 2.1202 - val_accuracy: 0.5237\n",
      "Epoch 9/20\n",
      "60/60 [==============================] - 5s 83ms/step - loss: 2.1142 - accuracy: 0.5058 - val_loss: 2.0536 - val_accuracy: 0.5214\n",
      "Epoch 10/20\n",
      "60/60 [==============================] - 5s 84ms/step - loss: 2.0573 - accuracy: 0.5132 - val_loss: 1.9985 - val_accuracy: 0.5220\n",
      "Epoch 11/20\n",
      "60/60 [==============================] - 5s 85ms/step - loss: 2.0017 - accuracy: 0.5143 - val_loss: 1.9524 - val_accuracy: 0.5248\n",
      "Epoch 12/20\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 1.9593 - accuracy: 0.5183 - val_loss: 1.9131 - val_accuracy: 0.5270\n",
      "Epoch 13/20\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 1.9282 - accuracy: 0.5293 - val_loss: 1.8794 - val_accuracy: 0.5331\n",
      "Epoch 14/20\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 1.8920 - accuracy: 0.5310 - val_loss: 1.8501 - val_accuracy: 0.5431\n",
      "Epoch 15/20\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 1.8689 - accuracy: 0.5418 - val_loss: 1.8242 - val_accuracy: 0.5487\n",
      "Epoch 16/20\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 1.8407 - accuracy: 0.5457 - val_loss: 1.8008 - val_accuracy: 0.5554\n",
      "Epoch 17/20\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 1.8126 - accuracy: 0.5560 - val_loss: 1.7793 - val_accuracy: 0.5587\n",
      "Epoch 18/20\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 1.7990 - accuracy: 0.5599 - val_loss: 1.7598 - val_accuracy: 0.5654\n",
      "Epoch 19/20\n",
      "60/60 [==============================] - 5s 83ms/step - loss: 1.7690 - accuracy: 0.5699 - val_loss: 1.7415 - val_accuracy: 0.5693\n",
      "Epoch 20/20\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 1.7579 - accuracy: 0.5709 - val_loss: 1.7246 - val_accuracy: 0.5793\n",
      "Epoch 1/20\n",
      "514/514 [==============================] - 5967s 12s/step - loss: 2.2619 - accuracy: 0.4889 - val_loss: 1.7178 - val_accuracy: 0.5910\n",
      "Epoch 2/20\n",
      "514/514 [==============================] - 21s 41ms/step - loss: 1.6164 - accuracy: 0.6175 - val_loss: 1.4896 - val_accuracy: 0.6694\n",
      "Epoch 3/20\n",
      "514/514 [==============================] - 14s 27ms/step - loss: 1.4275 - accuracy: 0.6754 - val_loss: 1.3587 - val_accuracy: 0.6939\n",
      "Epoch 4/20\n",
      "514/514 [==============================] - 12s 23ms/step - loss: 1.2884 - accuracy: 0.7102 - val_loss: 1.2671 - val_accuracy: 0.7145\n",
      "Epoch 5/20\n",
      "514/514 [==============================] - 11s 22ms/step - loss: 1.1840 - accuracy: 0.7319 - val_loss: 1.2016 - val_accuracy: 0.7390\n",
      "Epoch 6/20\n",
      "514/514 [==============================] - 13s 25ms/step - loss: 1.1024 - accuracy: 0.7505 - val_loss: 1.1513 - val_accuracy: 0.7490\n",
      "Epoch 7/20\n",
      "514/514 [==============================] - 14s 26ms/step - loss: 1.0238 - accuracy: 0.7749 - val_loss: 1.1093 - val_accuracy: 0.7563\n",
      "Epoch 8/20\n",
      "514/514 [==============================] - 11s 21ms/step - loss: 0.9686 - accuracy: 0.7865 - val_loss: 1.0744 - val_accuracy: 0.7668\n",
      "Epoch 9/20\n",
      "514/514 [==============================] - 11s 22ms/step - loss: 0.9189 - accuracy: 0.7932 - val_loss: 1.0502 - val_accuracy: 0.7696\n",
      "Epoch 10/20\n",
      "514/514 [==============================] - 11s 22ms/step - loss: 0.8771 - accuracy: 0.8006 - val_loss: 1.0210 - val_accuracy: 0.7774\n",
      "Epoch 11/20\n",
      "514/514 [==============================] - 9s 18ms/step - loss: 0.8206 - accuracy: 0.8149 - val_loss: 1.0010 - val_accuracy: 0.7769\n",
      "Epoch 12/20\n",
      "514/514 [==============================] - 9s 18ms/step - loss: 0.7797 - accuracy: 0.8237 - val_loss: 0.9803 - val_accuracy: 0.7824\n",
      "Epoch 13/20\n",
      "514/514 [==============================] - 9s 18ms/step - loss: 0.7457 - accuracy: 0.8333 - val_loss: 0.9704 - val_accuracy: 0.7841\n",
      "Epoch 14/20\n",
      "514/514 [==============================] - 10s 19ms/step - loss: 0.7118 - accuracy: 0.8388 - val_loss: 0.9462 - val_accuracy: 0.7902\n",
      "Epoch 15/20\n",
      "514/514 [==============================] - 12s 23ms/step - loss: 0.6747 - accuracy: 0.8511 - val_loss: 0.9414 - val_accuracy: 0.7913\n",
      "Epoch 16/20\n",
      "514/514 [==============================] - 11s 21ms/step - loss: 0.6487 - accuracy: 0.8550 - val_loss: 0.9245 - val_accuracy: 0.7991\n",
      "Epoch 17/20\n",
      "514/514 [==============================] - 11s 21ms/step - loss: 0.6206 - accuracy: 0.8607 - val_loss: 0.9115 - val_accuracy: 0.7974\n",
      "Epoch 18/20\n",
      "514/514 [==============================] - 11s 22ms/step - loss: 0.5959 - accuracy: 0.8695 - val_loss: 0.9019 - val_accuracy: 0.7991\n",
      "Epoch 19/20\n",
      "514/514 [==============================] - 15s 30ms/step - loss: 0.5728 - accuracy: 0.8707 - val_loss: 0.9030 - val_accuracy: 0.8013\n",
      "Epoch 20/20\n",
      "514/514 [==============================] - 15s 28ms/step - loss: 0.5459 - accuracy: 0.8824 - val_loss: 0.8937 - val_accuracy: 0.8036\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8296\\4199034334.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m best_parameter, best_accuracy = PSO(swarm_size = 10,\n\u001b[0m\u001b[0;32m      2\u001b[0m                                     \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                     \u001b[0mevaluation_funct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                     \u001b[0mlower_bound\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                     \u001b[0mupper_bound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8296\\3634769056.py\u001b[0m in \u001b[0;36mPSO\u001b[1;34m(swarm_size, dim, evaluation_funct, lower_bound, upper_bound, v_max, problem, max_iteration, termination_criteria)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mswarm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPSO_inizialization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mswarm_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluation_funct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower_bound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproblem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mbest_global_position\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPSO_alg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mswarm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower_bound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluation_funct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproblem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtermination_criteria\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# print('Best position found: ', best_global_position, 'with an evaluation of: ',global_opt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8296\\629950949.py\u001b[0m in \u001b[0;36mPSO_alg\u001b[1;34m(swarm, lower_bound, upper_bound, v_max, evaluation_funct, max_iteration, problem, termination_criteria, max_tol, correct_sol)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# set the termination criteria\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mcriteria_not_reach\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstop_criteria\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtermination_criteria\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswarm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_global_position\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswarm_position\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswarm_position\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswarm_diameter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswarm_diameter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaximum_tolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_tol\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mcriteria_not_reach\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8296\\2450011339.py\u001b[0m in \u001b[0;36mstop_criteria\u001b[1;34m(criteria, iteration, best_global_fitness, best_global_pos, maximum_tolerance, correct_sol, max_iteration, swarm_diameter, swarm_position)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mcriteria\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'fixed_iteration'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'swarm_radius'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mswarm_diameter\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswarm_position\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmax_iteration\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmax_iteration\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#if I have reached maximum number of iteration I stop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "best_parameter, best_accuracy = PSO(swarm_size = 10,\n",
    "                                    dim = 3,\n",
    "                                    evaluation_funct = MLP,\n",
    "                                    lower_bound=[0,0,0.0001],\n",
    "                                    upper_bound = [128,4,0.01],\n",
    "                                    v_max = [64,2,0.001],\n",
    "                                    problem = 'maximum',\n",
    "                                    max_iteration = 2,\n",
    "                                    termination_criteria = ['fixed_iteration', 'swarm_radius'],\n",
    "                                    max_tol = 0.1)\n",
    "\n",
    "print('Best position found: ', best_parameter, 'with an accuracy of: ',best_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
